@article{gregor2015draw,
  title={DRAW: A recurrent neural network for image generation},
  author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint, arXiv:1502.04623},
  year={2015},
  url={https://arxiv.org/pdf/1502.04623.pdf}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}
@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?ü¶ú},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@article{Yan22,
	abstract = {Information Extraction (IE) in Natural Language Processing (NLP) aims to extract structured information from unstructured text to assist a computer in understanding natural language. Machine learning-based IE methods bring more intelligence and possibilities but require an extensive and accurate labeled corpus. In the materials science domain, giving reliable labels is a laborious task that requires the efforts of many professionals. To reduce manual intervention and automatically generate materials corpus during IE, in this work, we propose a semi-supervised IE framework for materials via automatically generated corpus. Taking the superalloy data extraction in our previous work as an example, the proposed framework using Snorkel automatically labels the corpus containing property values. Then Ordered Neurons-Long Short-Term Memory (ON-LSTM) network is adopted to train an information extraction model on the generated corpus. The experimental results show that the F1-score of Œ≥'solvus temperature, density and solidus temperature of superalloys are 83.90{\%}, 94.02{\%}, 89.27{\%}, respectively. Furthermore, we conduct similar experiments on other materials, the experimental results show that the proposed framework is universal in the field of materials.},
	author = {Yan, Rongen and Jiang, Xue and Wang, Weiren and Dang, Depeng and Su, Yanjing},
	da = {2022/07/13},
	date-added = {2023-01-12 11:41:08 +0400},
	date-modified = {2023-01-12 11:41:08 +0400},
	doi = {10.1038/s41597-022-01492-2},
	id = {Yan2022},
	isbn = {2052-4463},
	journal = {Scientific Data},
	number = {1},
	pages = {401},
	title = {Materials information extraction via automatically generated corpus},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41597-022-01492-2},
	volume = {9},
	year = {2022},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41597-022-01492-2}}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{RAISSI2019686,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge‚ÄìKutta methods, Nonlinear dynamics}
}
@article{zaki22,
title = {Natural language processing-guided meta-analysis and structure factor database extraction from glass literature},
journal = {Journal of Non-Crystalline Solids: X},
volume = {15},
pages = {100103},
year = {2022},
issn = {2590-1591},
doi = {https://doi.org/10.1016/j.nocx.2022.100103},
url = {https://www.sciencedirect.com/science/article/pii/S2590159122000231},
author = {Mohd Zaki and Sahith Reddy Namireddy and Tanu Pittie and Vaibhav Bihani and Shweta Rani Keshri and Vineeth Venugopal and Nitya Nand Gosvami and  Jayadeva and N.M. Anoop Krishnan},
keywords = {Glasses, Structure factor, Natural language processing}
}
@inproceedings{ansley22,
author = {Finnie-Ansley, James and Denny, Paul and Becker, Brett A. and Luxton-Reilly, Andrew and Prather, James},
title = {The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming},
year = {2022},
isbn = {9781450396431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511861.3511863},
doi = {10.1145/3511861.3511863},
abstract = {Recent advances in artificial intelligence have been driven by an exponential growth in digitised data. Natural language processing, in particular, has been transformed by machine learning models such as OpenAI‚Äôs GPT-3 which generates human-like text so realistic that its developers have warned of the dangers of its misuse. In recent months OpenAI released Codex, a new deep learning model trained on Python code from more than 50 million GitHub repositories. Provided with a natural language description of a programming problem as input, Codex generates solution code as output. It can also explain (in English) input code, translate code between programming languages, and more. In this work, we explore how Codex performs on typical introductory programming problems. We report its performance on real questions taken from introductory programming exams and compare it to results from students who took these same exams under normal conditions, demonstrating that Codex outscores most students. We then explore how Codex handles subtle variations in problem wording using several published variants of the well-known ‚ÄúRainfall Problem‚Äù along with one unpublished variant we have used in our teaching. We find the model passes many test cases for all variants. We also explore how much variation there is in the Codex generated solutions, observing that an identical input prompt frequently leads to very different solutions in terms of algorithmic approach and code length. Finally, we discuss the implications that such technology will have for computing education as it continues to evolve, including both challenges and opportunities.},
booktitle = {Australasian Computing Education Conference},
pages = {10‚Äì19},
numpages = {10},
keywords = {AI, neural networks, copilot, machine learning, CS1, Codex, deep learning, novice programming, artificial intelligence, GPT-3, introductory programming, OpenAI, academic integrity, GitHub, code writing, code generation},
location = {Virtual Event, Australia},
series = {ACE '22}
}

@misc{Surya,
  title = {Surya: Foundation Model for Heliophysics},
  author = {Sujit Roy and Johannes Schmude and Rohit Lal and Vishal Gaur and Marcus Freitag and Julian Kuehnert and Theodore van Kessel and Dinesha V. Hegde and Andr√©s Mu√±oz-Jaramillo and Johannes Jakubik and Etienne Vos and Kshitiz Mandal and Ata Akbari Asanjan and Joao Lucas de Sousa Almeida and Amy Lin and Talwinder Singh and Kang Yang and Chetraj Pandey and Jinsu Hong and Berkay Aydin and Thorsten Kurth and Ryan McGranaghan and Spiridon Kasapis and Vishal Upendran and Shah Bahauddin and Daniel da Silva and Nikolai V. Pogorelov and Anne Spalding and Campbell Watson and Manil Maskey and Madhulika Guhathakurta and Juan Bernabe-Moreno and Rahul Ramachandran},
  abstract = {Heliophysics is central to understanding and forecasting space weather events and solar activity. Despite decades of high-resolution observations from the Solar Dynamics Observatory (SDO), most models remain task-specific and constrained by scarce labeled data, limiting their capacity to generalize across solar phenomena. We introduce Surya, a 366M parameter foundation model for heliophysics designed to learn general-purpose solar representations from multi-instrument SDO observations, including eight Atmospheric Imaging Assembly (AIA) channels and five Helioseismic and Magnetic Imager (HMI) products. Surya employs a spatiotemporal transformer architecture with spectral gating and long--short range attention, pretrained on high-resolution solar image forecasting tasks and further optimized through autoregressive rollout tuning. Zero-shot evaluations demonstrate its ability to forecast solar dynamics and flare events, while downstream fine-tuning with parameter-efficient Low-Rank Adaptation (LoRA) shows strong performance on solar wind forecasting, active region segmentation, solar flare forecasting, and EUV spectra. Surya is the first foundation model in heliophysics that uses time advancement as a pretext task on full-resolution SDO data. Its novel architecture and performance suggest that the model is able to learn the underlying physics behind solar evolution.},
  howpublished = {arXiv preprint arXiv:2508.14112},
  year = {2025},
  url = {https://arxiv.org/abs/2508.14112}
}

@article{JAROLIM2023102323,
  title = {Probing the Solar Coronal Magnetic Field with Physics-Informed Neural Networks},
  author = {Jarolim, R. and Thalmann, J. K. and Veronig, A. M. and Podladchikova, T.},
  date = {2023-10},
  journaltitle = {Nature Astronomy},
  shortjournal = {Nat Astron},
  volume = {7},
  number = {10},
  pages = {1171--1179},
  publisher = {Nature Publishing Group},
  issn = {2397-3366},
  doi = {10.1038/s41550-023-02030-9},
  url = {https://www.nature.com/articles/s41550-023-02030-9},
  urldate = {2025-11-12},
  abstract = {While the photospheric magnetic field of our Sun is routinely measured, its extent into the upper atmosphere is typically not accessible by direct observations. Here we present an approach for coronal magnetic-field extrapolation, using a neural network that integrates observational data and the physical force-free magnetic-field model. Our method flexibly finds a trade-off between the observation and force-free magnetic-field assumption, improving the understanding of the connection between the observation and the underlying physics. We utilize meta-learning concepts to simulate the evolution of active region NOAA\,11158. Our simulation of 5\,days of observations at full cadence (12\,minutes) requires less than 12\,hours of total computation time, allowing for real-time force-free magnetic-field extrapolations. The application to an analytical magnetic-field solution, a systematic analysis of the time evolution of free magnetic energy and magnetic helicity in the coronal volume, as well as comparison with extreme-ultraviolet observations, demonstrates the validity of our approach. The obtained temporal and spatial depletion of free magnetic energy unambiguously relates to the observed flare activity.},
  langid = {english},
  keywords = {Astrophysical magnetic fields,Computational astrophysics,Computer science,Solar physics},
  file = {/Users/u0167590/Zotero/storage/2B54AWXL/Jarolim et al. - 2023 - Probing the solar coronal magnetic field with physics-informed neural networks.pdf}
}