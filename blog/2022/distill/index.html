<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="google-site-verification=E4KIsiGdkAKwCvDyL7iYIfmitMwtc7hWhBdUV4jZX5M"> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Tensorflow tutorial for Physics Informed Neural Networks | George Miloshevich</title> <meta name="author" content="George Miloshevich"> <meta name="description" content="Some notes that explain how PINNs are implemented in tensorflow"> <meta name="keywords" content="George Miloshevich, Physicist, Machine Learning, Climate, Scientist"> <meta property="og:site_name" content="George Miloshevich"> <meta property="og:type" content="website"> <meta property="og:title" content="George Miloshevich | Tensorflow tutorial for Physics Informed Neural Networks"> <meta property="og:url" content="https://georgemilosh.github.io/blog/2022/distill/"> <meta property="og:description" content="Some notes that explain how PINNs are implemented in tensorflow"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Tensorflow tutorial for Physics Informed Neural Networks"> <meta name="twitter:description" content="Some notes that explain how PINNs are implemented in tensorflow"> <meta name="twitter:site" content="@george_milosh"> <meta name="twitter:creator" content="@george_milosh"> <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "George  Miloshevich"
        },
        "url": "https://georgemilosh.github.io/blog/2022/distill/",
        "@type": "WebSite",
        "description": "Some notes that explain how PINNs are implemented in tensorflow",
        "headline": "Tensorflow tutorial for Physics Informed Neural Networks",
        "sameAs": ["https://orcid.org/https://orcid.org/0000-0001-9896-1704", "https://scholar.google.com/citations?user=QMFVv0AAAAAJ", "https://github.com/georgemilosh", "https://www.linkedin.com/in/george-miloshevich-2b834212a", "https://twitter.com/george_milosh", "https://medium.com/@georgemilosh"],
        "name": "George  Miloshevich",
        "@context": "https://schema.org"
      }
    </script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://georgemilosh.github.io/blog/2022/distill/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Tensorflow tutorial for Physics Informed Neural Networks",
      "description": "Some notes that explain how PINNs are implemented in tensorflow",
      "published": "October 29, 2022",
      "authors": [
        {
          "author": "George Miloshevich",
          "authorURL": "georgemilosh.github.io",
          "affiliations": [
            {
              "name": "LSCE, CEA Saclay",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">GeorgeÂ </span>Miloshevich</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/repositories/">repositories</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/teaching/">teaching</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Tensorflow tutorial for Physics Informed Neural Networks</h1> <p>Some notes that explain how PINNs are implemented in tensorflow</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#pinns-intro">PINNs intro</a></div> <div><a href="#tensorflow-implementation">Tensorflow implementation</a></div> </nav> </d-contents> <p><strong>NOTE:</strong> This blog post contains some useful explanations for people who are less familiar with tensorflow, it explains few steps in a <a href="https://github.com/janblechschmidt/PDEsByNNs/blob/main/PINN_Solver.ipynb" rel="external nofollow noopener" target="_blank">notebook of Jan Blechschmidt</a></p> <h2 id="pinns-intro">PINNs intro</h2> <p>Physics Informed Neural Networks (PINNs) <d-cite key="RAISSI2019686"></d-cite> aim to solve Partial Differential Equatipons (PDEs) using neural networks. The crucial concept is to put the PDE into the loss, which is why they are referred to as <code class="language-plaintext highlighter-rouge">physics informed</code> <d-footnote> many authors have used similar terms such as physics-based etc, in some cases this involves using appropriate architecture adjustments rather than penalizing loss</d-footnote>.</p> <p>The essential thing to understand is that PINNs have been designed as a tool for two types of tasks, forward and inverse problem. Forward problem refers to the solutions of Partial Differential Equation (PDE) with known functional form and inverse problems refers to finding coefficients from data. The latter application is actually more interesting, since, as it turns out, solving PDEs with PINNs (former application) is generally more efficient when using standard numerical schemes and libraries.</p> <p>The method constructs a neural network approximation</p> \[u_\theta(t,x) \approx u(t,x)\] <p>of the solution of nonlinear PDE, where $u_\theta :[0,T] \times \mathcal{D} \to \mathbb{R}$ denotes a function realized by a neural network with parameters $\theta$.</p> <p>The continuous time approach for the parabolic PDE as described in (<a href="https://arxiv.org/abs/1711.10561" rel="external nofollow noopener" target="_blank">Raissi et al., 2017 (Part I)</a>) is based on the (strong) residual of a given neural network approximation $u_\theta \colon [0,T] \times \mathcal{D} \to \mathbb{R} $ of the solution $u$, i.e.,</p> \[\begin{align} r_\theta (t,x) := \partial_t u_\theta (t,x) + \mathcal{N}[u_\theta] (t,x). \end{align}\] <p>To incorporate this PDE residual $r_\theta$ into a loss function to be minimized, PINNs require a further differentiation to evaluate the differential operators $\partial_t u_\theta$ and $\mathcal{N}[u_\theta]$. Thus the PINN term $r_\theta$ shares the same parameters as the original network $u_\theta(t,x)$, but respects the underlying âphysicsâ of the nonlinear PDE. Both types of derivatives can be easily determined through automatic differentiation with current state-of-the-art machine learning libraries, e.g., TensorFlow or PyTorch.</p> <p>The PINN approach for the solution of the initial and boundary value problem now proceeds by minimization of the loss functional</p> \[\begin{align} \phi_\theta(X) := \phi_\theta^r(X^r) + \phi_\theta^0(X^0) + \phi_\theta^b(X^b), \end{align}\] <p>where $X$ denotes the collection of training data and the loss function $\phi_\theta$ contains the following terms:</p> <ul> <li>the mean squared residual \(\begin{align*} \phi_\theta^r(X^r) := \frac{1}{N_r}\sum_{i=1}^{N_r} \left|r_\theta\left(t_i^r, x_i^r\right)\right|^2 \end{align*}\) in a number of collocation points \(X^r:=\{(t_i^r, x_i^r)\}_{i=1}^{N_r} \subset (0,T] \times \mathcal{D}\), where \(r_\theta\) is the physics-informed neural network,</li> <li>the mean squared error with respect to the initial and boundary conditions \(\begin{align*} \phi_\theta^0(X^0) := \frac{1}{N_0} \sum_{i=1}^{N_0} \left|u_\theta\left(t_i^0, x_i^0\right) - u_0\left(x_i^0\right)\right|^2 \end{align*}\) and \(\begin{align*} \phi_\theta^b(X^b) := \frac{1}{N_b} \sum_{i=1}^{N_b} \left|u_\theta\left(t_i^b, x_i^b\right) - u_b\left(t_i^b, x_i^b\right)\right|^2 \end{align*}\)</li> </ul> <p>in a number of points \(X^0:=\{(t^0_i,x^0_i)\}_{i=1}^{N_0} \subset \{0\} \times \mathcal{D}\) and \(X^b:=\{(t^b_i,x^b_i)\}_{i=1}^{N_b} \subset (0,T] \times \partial \mathcal{D}\), where \(u_\theta\) is the neural network approximation of the solution \(u\colon[0,T] \times \mathcal{D} \to \mathbb{R}\).</p> <p>Note that the training data $X$ consists entirely of time-space coordinates.</p> <hr> <h2 id="tensorflow-implementation">Tensorflow implementation</h2> <p>I tested the notes given in <a href="https://github.com/janblechschmidt/PDEsByNNs/blob/main/PINN_Solver.ipynb" rel="external nofollow noopener" target="_blank">notebook of Jan Blechschmidt</a> I did not find significant increases in computation time using M1 Apple Mac book, despite lower specificaitons (on Google Colab this took noticeably longer). I am using <code class="language-plaintext highlighter-rouge">tensorflow-macos</code> and <code class="language-plaintext highlighter-rouge">tensorflow-metal</code> which is supposedly tuned for M1 processors. The task is relatively simple, so no hicaps observed during training, no heating or noise, while running in the battery mode. Below i just treat a very simple example that explains how the code works. We start by loading the required dependencies: the scientific computing library <a href="https://numpy.org/doc/stable/user/whatisnumpy.html" rel="external nofollow noopener" target="_blank">NumPy</a> and the machine learning library <a href="https://www.tensorflow.org/" rel="external nofollow noopener" target="_blank">TensorFlow</a> and also work with single precision.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
  <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
  
  <span class="c1"># Set data type
</span>  <span class="n">DTYPE</span><span class="o">=</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span>
  <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">backend</span><span class="p">.</span><span class="nf">set_floatx</span><span class="p">(</span><span class="n">DTYPE</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Note:</strong> Below we will give more clear explanation of how the method will work by creating simple network using just 1 hidden layer and exponential activation function to be able to easily take gradients</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define residual of the PDE
</span>
<span class="k">def</span> <span class="nf">init_model_simple</span><span class="p">(</span><span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_neurons_per_layer</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="c1"># Initialize a feedforward neural network
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">()</span>

    <span class="c1"># Input is one-dimensional (time + one spatial dimension)
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Append hidden layers
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_hidden_layers</span><span class="p">):</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">num_neurons_per_layer</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">activations</span><span class="p">.</span><span class="n">exponential</span><span class="p">,</span>
            <span class="n">kernel_initializer</span><span class="o">=</span><span class="sh">'</span><span class="s">glorot_normal</span><span class="sh">'</span><span class="p">))</span>

    <span class="c1"># Output is one-dimensional
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">model</span>

<span class="n">model_simple</span> <span class="o">=</span> <span class="nf">init_model_simple</span><span class="p">()</span>
</code></pre></div></div> <p>To show how the algorithm works I only evaluate $du/dx$ and extract the weights of the relevant weights. Notice that with glorot initialization biases are zero and weights are random normal.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_r_simple</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># A tf.GradientTape is used to compute derivatives in TensorFlow
</span>    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">tape</span><span class="p">.</span><span class="nf">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Determine residual 
</span>        <span class="n">u</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Compute gradient u_x within the GradientTape
</span>        <span class="c1"># since we need second derivatives
</span>        <span class="n">u_x</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">del</span> <span class="n">tape</span>

    <span class="k">return</span> <span class="n">u_x</span>

<span class="n">w21</span> <span class="o">=</span> <span class="n">model_simple</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="nf">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># last layer
</span><span class="n">w22</span> <span class="o">=</span> <span class="n">model_simple</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="nf">numpy</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">w11</span> <span class="o">=</span> <span class="n">model_simple</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># first layer
</span><span class="n">w12</span> <span class="o">=</span> <span class="n">model_simple</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">du/dx = </span><span class="si">{</span><span class="nf">get_r_simple</span><span class="p">(</span><span class="n">model_simple</span><span class="p">,</span> 
  <span class="n">tf</span><span class="p">.</span><span class="nf">constant</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)).</span><span class="nf">numpy</span><span class="p">()</span><span class="si">}</span><span class="s"> = 
  </span><span class="si">{</span><span class="n">w21</span><span class="o">*</span><span class="n">w11</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">w11</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">w22</span><span class="o">*</span><span class="n">w12</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">w12</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span> <span class="p">)</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div> <p>Indeed the $du/dx$ is computed consistently (within single precision) both by hand and using the network (run the network to confirm)</p> <p>Below we set up the loss $l := \partial_x u $ and compute its gradients (for simplicity of analytical manipulations we chose exponential activation function and just one hidden layer with two neurons)</p> \[\begin{aligned} u = w_{21}\,\exp(w_{11}\, x + b_{11}) + b_{21} + w_{22}\,\exp(w_{12}\, x + b_{12}) + b_{22}\\ l := \partial_x u = w_{21}\,w_{11}\,\exp(w_{11} \, x + b_{11}) + w_{22}\,w_{12}\,\exp(w_{12}\, x + b_{12}) \end{aligned}\] <p>We will take very simple loss as stated, which will depend only on $du/dx$</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_loss_simple</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_r</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">get_r_simple</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_r</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_grad_simple</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_r</span><span class="p">):</span>
    
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="c1"># This tape is for derivatives with
</span>        <span class="c1"># respect to trainable variables
</span>        <span class="c1">#tape.watch(model.trainable_variables)
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="nf">compute_loss_simple</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_r</span><span class="p">)</span>

    <span class="n">g</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">tape</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">g</span>

<span class="n">loss</span><span class="p">,</span> <span class="n">g</span> <span class="o">=</span> <span class="nf">get_grad_simple</span><span class="p">(</span><span class="n">model_simple</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gi</span><span class="p">,</span> <span class="n">varsi</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">model_simple</span><span class="p">.</span><span class="n">variables</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">varsi</span><span class="p">.</span><span class="n">name</span><span class="si">}</span><span class="s"> has graidents </span><span class="si">{</span><span class="n">gi</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>This generates output which I put in the following table (your results will depend on itialization)</p> <table> <thead> <tr> <th>Layer name</th> <th style="text-align: center">neuron 1</th> <th style="text-align: right">neuron 2.</th> </tr> </thead> <tbody> <tr> <td>dense_2/kernel:0</td> <td style="text-align: center">4.466523</td> <td style="text-align: right">-0.13126281</td> </tr> <tr> <td>dense_2/bias:0</td> <td style="text-align: center">2.0364249</td> <td style="text-align: right">-0.00236067</td> </tr> <tr> <td>dense_3/kernel:0</td> <td style="text-align: center">1.9372414</td> <td style="text-align: right">0.01865212</td> </tr> <tr> <td>dense_3/bias:0</td> <td style="text-align: center">1.9372414</td> <td style="text-align: right">0.01865212</td> </tr> </tbody> </table> <p>It has the following expressions given that $l = w_{21}\,w_{11}\,\exp(w_{11} \, x + b_{11}) + w_{22}\,w_{12}\,\exp(w_{12}\, x + b_{12})$ \(\begin{aligned} \frac{\partial \, l}{\partial {b_{11}}} = w_{21}\,w_{11}\,\exp(w_{11} \, x + b_{11}) \\ \frac{\partial \, l}{\partial {w_{12}}} = w_{22} (1+w_{12}^2)\,\exp(w_{12}\, x + b_{12}) \end{aligned}\)</p> <p>and othersâ¦</p> <p>Note that in $\partial_x u$ there is no derivatives with respect to the biases in the last layer: $b_{21}$ and $b_{22}$, this is why the output of the list had None as the last element (because it shows gradients with respect to the bias in the last layer). We will compare the values below, to the ones evaluate by hand. You should be able to get consistent results with the lines below</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The value we expect for dl/dw11 = </span><span class="si">{</span><span class="n">w21</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">w11</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">w11</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s">, dl/dw12 = </span><span class="si">{</span><span class="n">w22</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">w12</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">w12</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The value we expect for dl/db11 = </span><span class="si">{</span><span class="n">w21</span><span class="o">*</span><span class="n">w11</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">w11</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s">, dl/db12 = </span><span class="si">{</span><span class="n">w22</span><span class="o">*</span><span class="n">w12</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">w12</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The value we expect for dl/dw21 = </span><span class="si">{</span><span class="n">w11</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">w11</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s">, dl/dw22 = </span><span class="si">{</span><span class="n">w12</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">w12</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"georgemilosh/georgemilosh.github.io","data-repo-id":"R_kgDOGhoqGg==","data-category":"General","data-category-id":"DIC_kwDOGhoqGs4CTi-m","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2025 George Miloshevich. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>