<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="google-site-verification=E4KIsiGdkAKwCvDyL7iYIfmitMwtc7hWhBdUV4jZX5M"> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Tensorflow tutorial for Physics Informed Neural Networks | George Miloshevich</title> <meta name="author" content="George Miloshevich"> <meta name="description" content="Some notes that explain how PINNs are implemented in tensorflow"> <meta name="keywords" content="George Miloshevich, Physicist, Machine Learning, Climate, Scientist"> <meta property="og:site_name" content="George Miloshevich"> <meta property="og:type" content="website"> <meta property="og:title" content="George Miloshevich | Tensorflow tutorial for Physics Informed Neural Networks"> <meta property="og:url" content="https://georgemilosh.github.io/blog/2022/distill/"> <meta property="og:description" content="Some notes that explain how PINNs are implemented in tensorflow"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Tensorflow tutorial for Physics Informed Neural Networks"> <meta name="twitter:description" content="Some notes that explain how PINNs are implemented in tensorflow"> <meta name="twitter:site" content="@george_milosh"> <meta name="twitter:creator" content="@george_milosh"> <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "George  Miloshevich"
        },
        "url": "https://georgemilosh.github.io/blog/2022/distill/",
        "@type": "WebSite",
        "description": "Some notes that explain how PINNs are implemented in tensorflow",
        "headline": "Tensorflow tutorial for Physics Informed Neural Networks",
        "sameAs": ["https://orcid.org/https://orcid.org/0000-0001-9896-1704", "https://scholar.google.com/citations?user=QMFVv0AAAAAJ", "https://github.com/georgemilosh", "https://www.linkedin.com/in/george-miloshevich-2b834212a", "https://twitter.com/george_milosh"],
        "name": "George  Miloshevich",
        "@context": "https://schema.org"
      }
    </script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://georgemilosh.github.io/blog/2022/distill/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Tensorflow tutorial for Physics Informed Neural Networks",
      "description": "Some notes that explain how PINNs are implemented in tensorflow",
      "published": "October 29, 2022",
      "authors": [
        {
          "author": "George Miloshevich",
          "authorURL": "georgemilosh.github.io",
          "affiliations": [
            {
              "name": "LSCE, CEA Saclay",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">George </span>Miloshevich</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/repositories/">repositories</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/teaching/">teaching</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Tensorflow tutorial for Physics Informed Neural Networks</h1> <p>Some notes that explain how PINNs are implemented in tensorflow</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#pinns-intro">PINNs intro</a></div> <div><a href="#tensorflow-implementation">Tensorflow implementation</a></div> </nav> </d-contents> <p><strong>NOTE:</strong> This blog post contains some useful explanations for people who are less familiar with tensorflow, it explains few steps in a <a href="https://github.com/janblechschmidt/PDEsByNNs/blob/main/PINN_Solver.ipynb" rel="external nofollow noopener" target="_blank">notebook of Jan Blechschmidt</a> which should be consulted for details and whose code I re-use below.</p> <h2 id="pinns-intro">PINNs intro</h2> <p>Physics Informed Neural Networks (PINNs) <d-cite key="RAISSI2019686"></d-cite> aim to solve Partial Differential Equatipons (PDEs) using neural networks. The crucial concept is to put the PDE into the loss, which is why they are referred to as <code class="language-plaintext highlighter-rouge">physics informed</code> <d-footnote> many authors have used similar terms such as physics-based etc, in some cases this involves using appropriate architecture adjustments rather than penalizing loss</d-footnote>.</p> <p>The method constructs a neural network approximation</p> \[u_\theta(t,x) \approx u(t,x)\] <p>of the solution of nonlinear PDE, where $u_\theta :[0,T] \times \mathcal{D} \to \mathbb{R}$ denotes a function realized by a neural network with parameters $\theta$.</p> <p>The continuous time approach for the parabolic PDE as described in (<a href="https://arxiv.org/abs/1711.10561" rel="external nofollow noopener" target="_blank">Raissi et al., 2017 (Part I)</a>) is based on the (strong) residual of a given neural network approximation $u_\theta \colon [0,T] \times \mathcal{D} \to \mathbb{R} $ of the solution $u$, i.e.,</p> \[\begin{align} r_\theta (t,x) := \partial_t u_\theta (t,x) + \mathcal{N}[u_\theta] (t,x). \end{align}\] <p>To incorporate this PDE residual $r_\theta$ into a loss function to be minimized, PINNs require a further differentiation to evaluate the differential operators $\partial_t u_\theta$ and $\mathcal{N}[u_\theta]$. Thus the PINN term $r_\theta$ shares the same parameters as the original network $u_\theta(t,x)$, but respects the underlying “physics” of the nonlinear PDE. Both types of derivatives can be easily determined through automatic differentiation with current state-of-the-art machine learning libraries, e.g., TensorFlow or PyTorch.</p> <p>The PINN approach for the solution of the initial and boundary value problem now proceeds by minimization of the loss functional</p> \[\begin{align} \phi_\theta(X) := \phi_\theta^r(X^r) + \phi_\theta^0(X^0) + \phi_\theta^b(X^b), \end{align}\] <p>where $X$ denotes the collection of training data and the loss function $\phi_\theta$ contains the following terms:</p> <ul> <li>the mean squared residual \(\begin{align*} \phi_\theta^r(X^r) := \frac{1}{N_r}\sum_{i=1}^{N_r} \left|r_\theta\left(t_i^r, x_i^r\right)\right|^2 \end{align*}\) in a number of collocation points \(X^r:=\{(t_i^r, x_i^r)\}_{i=1}^{N_r} \subset (0,T] \times \mathcal{D}\), where \(r_\theta\) is the physics-informed neural network,</li> <li>the mean squared error with respect to the initial and boundary conditions \(\begin{align*} \phi_\theta^0(X^0) := \frac{1}{N_0} \sum_{i=1}^{N_0} \left|u_\theta\left(t_i^0, x_i^0\right) - u_0\left(x_i^0\right)\right|^2 \quad \text{ and } \\ \phi_\theta^b(X^b) := \frac{1}{N_b} \sum_{i=1}^{N_b} \left|u_\theta\left(t_i^b, x_i^b\right) - u_b\left(t_i^b, x_i^b\right)\right|^2 \end{align*}\) in a number of points \(X^0:=\{(t^0_i,x^0_i)\}_{i=1}^{N_0} \subset \{0\} \times \mathcal{D}\) and \(X^b:=\{(t^b_i,x^b_i)\}_{i=1}^{N_b} \subset (0,T] \times \partial \mathcal{D}\), where \(u_\theta\) is the neural network approximation of the solution \(u\colon[0,T] \times \mathcal{D} \to \mathbb{R}\).</li> </ul> <p>Note that the training data $X$ consists entirely of time-space coordinates.</p> <hr> <h2 id="tensorflow-implementation">Tensorflow implementation</h2> <p>I tested the notes given in <a href="https://github.com/janblechschmidt/PDEsByNNs/blob/main/PINN_Solver.ipynb" rel="external nofollow noopener" target="_blank">notebook of Jan Blechschmidt</a> I did not find significant incrases in computation time using M1 Apple Mac book, despite lower specificaitons (on Google Colab this took noticeably longer). I am using <code class="language-plaintext highlighter-rouge">tensorflow-macos</code> and <code class="language-plaintext highlighter-rouge">tensorflow-metal</code> which is supposedly tuned for M1 processors. The task is relatively simple, so no hicaps observed during training, no heating or noise, while running in the battery mode. Below i just treat a very simple example that explains how the code works. We start by loading the required dependencies: the scientific computing library <a href="https://numpy.org/doc/stable/user/whatisnumpy.html" rel="external nofollow noopener" target="_blank">NumPy</a> and the machine learning library <a href="https://www.tensorflow.org/" rel="external nofollow noopener" target="_blank">TensorFlow</a> and also work with single precision.</p> <d-code block="" language="python"> # Import TensorFlow and NumPy import tensorflow as tf import numpy as np # Set data type DTYPE='float32' tf.keras.backend.set_floatx(DTYPE) </d-code> <p><strong>Note:</strong> Below we will give more clear explanation of how the method will work by creating simple network using just 1 hidden layer and exponential activation function to be able to easily take gradients</p> <d-code block="" language="python"> # Define residual of the PDE def init_model_simple(num_hidden_layers=1, num_neurons_per_layer=2): # Initialize a feedforward neural network model = tf.keras.Sequential() # Input is one-dimensional (time + one spatial dimension) model.add(tf.keras.Input(1)) # Append hidden layers for _ in range(num_hidden_layers): model.add(tf.keras.layers.Dense(num_neurons_per_layer, activation=tf.keras.activations.exponential, kernel_initializer='glorot_normal')) # Output is one-dimensional model.add(tf.keras.layers.Dense(1)) return model model_simple = init_model_simple() </d-code> <p>To show how the algorithm works I only evaluate $du/dx$ and extract the weights of the relevant weights. Notice that with glorot initialization biases are zero and weights are random normal.</p> <d-code block="" language="python"> def get_r_simple(model, x): # A tf.GradientTape is used to compute derivatives in TensorFlow with tf.GradientTape(persistent=True) as tape: tape.watch(x) # Determine residual u = model(x) # Compute gradient u_x within the GradientTape # since we need second derivatives u_x = tape.gradient(u, x) del tape return u_x w21 = model_simple.weights[2].numpy()[0] # last layer w22 = model_simple.weights[2].numpy()[1] w11 = model_simple.weights[0].numpy()[0,0] # first layer w12 = model_simple.weights[0].numpy()[0,1] x = tf.constant([1], dtype=tf.float32) print(f'du/dx = {get_r_simple(model_simple, tf.constant(x, dtype=tf.float32)).numpy()} = {w21*w11*np.exp(w11*x) + w22*w12*np.exp(w12)}' ) print s </d-code> <p>Indeed the $du/dx$ is computed consistently (within single precision) both by hand and using the network (run the network to confirm)</p> <p>Below we set up the loss $l := \partial_x u $ and compute its gradients (for simplicity of analytical manipulations we chose exponential activation function and just one hidden layer with two neurons)</p> \[\begin{aligned} u = w_{21}\,\exp(w_{11}\, x + b_{11}) + b_{21} + w_{22}\,\exp(w_{12}\, x + b_{12}) + b_{22}\\ l := \partial_x u = w_{21}\,w_{11}\,\exp(w_{11} \, x + b_{11}) + w_{22}\,w_{12}\,\exp(w_{12}\, x + b_{12}) \end{aligned}\] <p>We will take very simple loss as stated, which will depend only on $du/dx$</p> <d-code block="" language="python"> def compute_loss_simple(model, X_r): return get_r_simple(model, X_r) def get_grad_simple(model, X_r): with tf.GradientTape(persistent=True) as tape: # This tape is for derivatives with # respect to trainable variables #tape.watch(model.trainable_variables) loss = compute_loss_simple(model, X_r) g = tape.gradient(loss, model.trainable_variables) del tape return loss, g loss, g = get_grad_simple(model_simple, x) for gi, varsi in zip(g, model_simple.variables): print(f'{varsi.name} has graidents {gi}') </d-code> <p>This generates output which I put in the following table (your results will depend on itialization)</p> <table> <thead> <tr> <th>Layer name</th> <th style="text-align: center">neuron 1</th> <th style="text-align: right">neuron 2.</th> </tr> </thead> <tbody> <tr> <td>dense_2/kernel:0</td> <td style="text-align: center">4.466523</td> <td style="text-align: right">-0.13126281</td> </tr> <tr> <td>dense_2/bias:0</td> <td style="text-align: center">2.0364249</td> <td style="text-align: right">-0.00236067</td> </tr> <tr> <td>dense_3/kernel:0</td> <td style="text-align: center">1.9372414</td> <td style="text-align: right">0.01865212</td> </tr> <tr> <td>dense_3/bias:0</td> <td style="text-align: center">1.9372414</td> <td style="text-align: right">0.01865212</td> </tr> </tbody> </table> <p>It has the following expressions given that $l = w_{21}\,w_{11}\,\exp(w_{11} \, x + b_{11}) + w_{22}\,w_{12}\,\exp(w_{12}\, x + b_{12})$ \(\begin{aligned} \frac{\partial \, l}{\partial {b_{11}}} = w_{21}\,w_{11}\,\exp(w_{11} \, x + b_{11}) \\ \frac{\partial \, l}{\partial {w_{12}}} = w_{22} (1+w_{12}^2)\,\exp(w_{12}\, x + b_{12}) \end{aligned}\)</p> <p>and others…</p> <p>Note that in $\partial_x u$ there is no derivatives with respect to the biases in the last layer: $b_{21}$ and $b_{22}$, this is why the output of the list had None as the last element (because it shows gradients with respect to the bias in the last layer). We will compare the values below, to the ones evaluate by hand. You should be able to get consistent results with the lines below</p> <d-code block="" language="python"> print(f"The value we expect for dl/dw11 = {w21*(1+w11**2)*np.exp(w11*x)}, dl/dw12 = {w22*(1+w12*2)*np.exp(w12*x)}") print(f"The value we expect for dl/db11 = {w21*w11*np.exp(w11*x)}, dl/db12 = {w22*w12*np.exp(w12*x)}") print(f"The value we expect for dl/dw21 = {w11*np.exp(w11*x)}, dl/dw22 = {w12*np.exp(w12*x)}") </d-code> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 George Miloshevich. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>