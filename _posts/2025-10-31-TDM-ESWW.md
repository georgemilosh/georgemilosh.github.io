---
layout: distill
title: AI in Space Weather
description: Scientific Outlooks for the Analysis of Space Weather Data in the Age of AI
giscus_comments: true
date: 2025-10-31
featured: true
related_posts: true

authors:
  - name: George Miloshevich
    url: "georgemilosh.github.io"
    affiliations:
      name: CmPA, KU Leuven
  - name: Panagiotis Gonidakis
    affiliations:
      name: CmPA, KU Leuven
  - name: Ekaterina Dineva
    url: "https://astrodineva.faculty.bio/"
    affiliations:
      name: CmPA, KU Leuven
 

bibliography: 2018-12-22-distill.bib

toc:
  - name: Topical Discussion Meeting Summary
  - name: Well-Curated, Transparent Datasets  
  - name: Physics-Informed Learning
  - name: Onboard Processing, Autonomy and Radiation-Resistant Hardware
  - name: Foundation models
  - name: Synthesis and Outlook

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

## Topical Discussion Meeting Summary

Together with my colleagues Panagiotis Gonidakis, [Ekaterina Dineva](https://astrodineva.faculty.bio/) and [Stefaan Poedts](https://perswww.kuleuven.be/~u0002601/), I had the pleasure of moderating and organizing the Topical Discussion Meeting [Scientific Outlooks for the Analysis of Space Weather Data in the Age of AI](https://events.spacepole.be/event/222/sessions/599/#20251030), held during the ESWW conference in Umeå. Our panelists, [Jasmina Magdalenic](https://www.kuleuven.be/wieiswie/en/person/00142602), [Hannah Theresa Rüdisser](https://hruedisser.github.io/), [Matthew West](https://www.solarnet-project.eu/matthew-j-west), and [Savvas Raptis](https://savvasraptis.github.io/), have covered a range of topics, from the transition of AI tools from research to operations, physics-informed learning, AI on board space missions, and foundation models. Below I summarise the key points from the discussion.

<div class="d-flex justify-content-center">
  <div>{% include figure.html path="assets/img/tdm1.jpeg" class="img-fluid rounded z-depth-1" %}</div>
</div>
<div class="caption">
    Left: George Miloshevich moderating the panel discussion. Right: Panelists Jasmina Magdalenic, Hannah Theresa Rüdisser, Matthew West, and Ekaterina Dineva with Savvas Raptis joining online.
</div>

## Well-Curated, Transparent Datasets  
The foundation of any trustworthy AI system—or any scientific model—lies in its data. The speakers argued in favor of prioritising **well-curated, transparent datasets** and **reproducible workflows** as the bedrock upon which AI applications in space weather should be built. The dataset has been carefully selected, cleaned, annotated, documented and maintained so as to minimise errors, inconsistencies, and hidden biases.  [reference:lightly.ai](https://www.lightly.ai/blog/data-curation). By *reproducible workflows* we mean that all steps from raw measurement → preprocessing → model training → evaluation are documented, version controlled, and accessible so that others can replicate or build upon the work. Open data and open access is also policy of leading scientific ML journals such as [JGR: Machine Learning and Computation](https://agupubs.onlinelibrary.wiley.com/journal/29935210). The panel emphasised that without these foundations, even the most advanced AI or physics-informed model may be fragile when transferred to a new instrument, new solar cycle, or a different spatial regime.

## Physics-Informed Learning  
The notion of **physics-informed learning** (or physics-informed machine learning) offers a bridge between classical physical modelling and modern data-driven approaches.

<div class="d-flex justify-content-center">
  <div>{% include figure.html path="assets/img/savvas.jpeg" class="img-fluid rounded z-depth-1" %}</div>
</div>
<div class="caption">
    Savvas Raptis presenting online from John's Hopkins University Applied Physics Lab.
</div>

In brief, this means embedding known physical laws (for example conservation of energy, momentum, Maxwell’s equations, magneto-hydrodynamic approximations) into the machine-learning model architecture, training process or loss function. The idea is that rather than letting a neural network “learn everything from scratch” from large volumes of data, you constrain it (regularise it) with prior physical knowledge so that it learns plausible solutions more efficiently. For example, you might penalise solutions that violate a divergence-free magnetic field condition or that fail to conserve mass in a plasma flow. For more details on PINNs <d-cite key="RAISSI2019686"></d-cite> the blog post [Physics-Informed Neural Networks](https://georgemilosh.github.io/blog/2022/distill/). One potential highlight is the solar loop reconstruction with PINNs <d-cite key="JAROLIM2023102323"></d-cite>.

However, the panel also stressed limitations. In space‐plasma systems you often face *multi-scale behaviour*: for example, fluid‐scale (magneto-hydrodynamics) may apply in one region, while kinetic (particle) physics dominates elsewhere. If you impose a “one‐size fits all” physical model in your physics‐informed ML, you risk mismodelling regimes where the assumptions break down. In other words: physics‐informed learning is a powerful tool, *but only if your prior physics matches the regime of interest*. 

## Onboard Processing, Autonomy and Radiation-Resistant Hardware  
Thanks to Vigil mission scientist [Matthew West](https://www.solarnet-project.eu/matthew-j-west) more room was provided t

With growing data volumes from space missions and limited downlink bandwidth, there is increasing interest in **AI algorithms onboard spacecraft**, which enables near-real time decision-making (for example, deciding **which data to downlink**, or **automatic labeling of events**). This move is supported by increasingly radiation‐hardened or radiation-resistant hardware capable of running ML models in the harsh space environment such as FPGAs (Field Programmable Gate Arrays). This topic is currently under development in [ASAP project](https://asap-space.eu/).

Typically, selective downlink is performed by a mission scientist on the ground, who reviews low-resolution or summary data and decides which segments to request in high resolution. [Matthew West](https://www.solarnet-project.eu/matthew-j-west) has mentioned that missions such as the Vigil mission were pointed to as platforms where multi-level data access (raw → processed → meta-data) combined with onboard intelligence may increase operational and scientific yield in the future Vigil missions.

## Foundation models

<div class="d-flex justify-content-center">
  <div>{% include figure.html path="assets/img/rs=w-1280.webp" class="img-fluid rounded z-depth-1" %}</div>
</div>
<div class="caption">
    Panagiotis Gonidakis presenting the speakers during the panel discussion.
</div>

**Foundation models** (FMs) are large machine-learning models pretrained on massive datasets using self-supervised or unsupervised methods, which can later be adapted (fine-tuned) for a variety of downstream tasks. In simple terms: build one big model that learns general patterns from broad data; then adapt it for specifics. That contrasts with building bespoke models for each task from scratch. In the context of space physics, FMs offer the promise of transferability: a model pretrained on a broad set of solar images might be fine-tuned for flare prediction, solar-wind forecasting or active-region segmentation. FMs may reduce the data requirement for each downstream task. Examples of foundation models in Solar Physics include Surya model <d-cite key="Surya"></d-cite>  although it was not mentioned during the panel discussion.

However, there are caveats:
	* The physical consistency of the pretrained model must be assessed: learning from broad data doesn’t guarantee that the model respects all relevant physics or works across scales.
	* In space physics, where certain regimes are rare (e.g., extreme substorms) or scales vary widely, the foundation model may still require careful fine-tuning or may even mis-generalise.
	* Transparency in the pretraining dataset, the architecture, and the fine-tuning pipelines is essential, especially if the FM is used for operational forecasting rather than purely research.
	* Because FMs are large and complex, it might become difficult for the end user to actually run such models in certain cases.

In the community discussion, participants agreed that while foundation models are exciting possibilities, they must be critically evaluated for true physical relevance and robustness across instrument sets, spatial regimes, and solar-cycle phases. They are not “plug-and-play magic.


## Synthesis and Outlook  

<div class="d-flex justify-content-center">
  <div>{% include figure.html path="assets/img/rs=w-12802.webp" class="img-fluid rounded z-depth-1" %}</div>
</div>
<div class="caption">
    Aurora Borealis spotted during European Space Weather Week in Umeå.
</div>

Physics-informed learning offers a way to reduce data demands and increase physical credibility—but only if the physics assumptions are valid for the regime in question. Event detection remains a challenging domain because of limited ground truth and subjective labelling; unsupervised methods and human–AI systems are key enablers. Onboard processing and autonomy promise to amplify scientific return, especially when bandwidth or latency is constrained, but they depend on robust hardware and trustworthy algorithms. Through all of this, transparency, reproducibility and open data remain the bedrock of credible science and operational utility.

Ultimately, AI in space‐physics should be viewed not as a replacement for human domain expertise, but as an **enhancement**—a tool that empowers scientists and operations engineers to make better decisions, discover new phenomena, and transition research into real-world practice with confidence.